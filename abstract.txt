This thesis presents a state-of-the-art approach to the time-intensive and complex task of testing graphical user interfaces (GUIs). Given the combinatorial nature of potential user inputs, GUIs often encompass infinite unique states, which cannot be sufficiently explored using exhaustive search-based methods. To address this, a reinforcement learning (RL) agent is proposed that mimics human testers, autonomously learning to interact with web application GUIs, thereby uncovering potential bugs and faults. The agent is solely provided with GUI screenshots and a reward signal that incentives the agent to gradually learn an efficient policy for exploring GUI elements. The applicability of several state-of-the-art RL algorithms, including Proximal Policy Optimisation (PPO) and Soft-Actor-Critic (SAC) is evaluated, and benchmarked against existing methods. The results demonstrate improvements upon existing RL- as well as human benchmarks, while simultaneously decreasing the training time. As a supplementary outcome of this thesis, a standard OpenAI Gym environment is developed and made available, facilitating future research and experimentation in web application GUI testing.